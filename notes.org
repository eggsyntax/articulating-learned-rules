Note -- the rest of this doc is written in emacs org mode and may be unpleasant to read in other environments, despite being text-only. However, it's only my own notes-to-self, which are not part of the public-facing output, and which I include solely for the sake of completeness.

*** Notes on 'articulating learned rules' project (MATS application, Owain Evans' stream, winter 2024-5).
**** Structure
***** Abstract
****** TODO rewrite
******* Current large language models can capably perform classification tasks on text given a description of the task and some example classifications. We investigate, first, whether they can do well on such classification tasks given only example classifications (ie without an actual description of the task). Second, for classification tasks that they succeed in performing well based only on examples, we investigate whether they can articulate the classification rule they have learned. We discover that in some cases they can and in others they cannot; we discuss possible framings of this discrepancy and whether we should expect such rule articulations to be faithful to the model's actual reasoning during classification.
***** Introduction
****** There are many reasons to be concerned about the safety of future AI models as their capabilities and intelligence continue to advance rapidly[2]. One key reason is the risk of misalignment, ie that these systems will have goals which differ from those we want them to have. A possible route for identifying misalignment is for models to accurately report their internal processes. Researchers have begun investigating models' capability for such introspection in depth[3][4].
****** One path for exploring introspection, which we take here, is to cause the model to internalize a particular goal which we specify in advance, and then ask it to articulate the internalized goal. This has the substantial advantage that we have the underlying ground truth of the goal to compare to.
****** For example, it has been known for several years [1] that large language models are capable of performing classification tasks, given only text instructions and some examples of correct classifications, and that this capability improves with model size. If we provide few-shot examples, we induce a new, internalized goal (since in-context learning is, at least on some accounts[5], equivalent to gradient descent in its effects on the model). If the model is then capable of articulating this internalized goal without having it described in advance**, this provides evidence that the model can at least sometimes articulate its internal goals, which can be quite useful for alignment purposes.
****** Of course, the validity of this evidence depends on the articulated goal being faithful to the model's actual reasoning during classification. We find that the model's articulation of its classification process does not always successfully match either the initial goal or the goal that the model implicitly uses for classification. In the discussion section we consider some possible framings for this discrepancy and their consequences. TODO and tests for distinguishing?
***** Related work (skippable)
***** Methodology
****** Model: GPT-4o (and one comparison with Claude-3.5-Sonnet)
****** Repository: https://github.com/eggsyntax/articulating-learned-rules
****** Each experiment proceeds in three steps:
******* First, we create a description of a classification task. As a running example, we'll use this one:
TODO
******** See Appendix A for the full prompt and output for this task.
******** We ask the model to generate the desired number of cases, along with their correct classifications. We also produce some extra lines in case there are errors in the resulting cases.
********* Typically we provide the model with 30 training cases (along with their correct classifications), and test against 20 test cases, using a separate instance for each test case. In one case we used 80 and 20; in another we used 170 and 30.
******** The generated cases are then examined manually for false positives and false negatives, and any such errors are replaced by one of the extra generated lines.
******** The remaining extra lines are discarded.
******** The prompt and the generated test cases are saved for later analysis.
******* Second, we have (another instance of) the model attempt to classify test cases.
******** For each test case, we present the set of training cases with correct classifications, then present the test case and ask for classification.
******** We take minor steps to extract the answer from the model's response (stripping whitespace, handling a few common cases like 'the answer is __'). Any invalid responses are recorded and discarded.
******** Correct, incorrect, and invalid classifications are reported, and accuracy statistics are generated.
******* Third, if the model is at least 90% accurate on the classification test, we ask it to articulate the rule it is using to do classification.
******** In short, the prompt asks, 'Based on these examples, please articulate the general rule or pattern you're using to determine the correct classification. Be specific about what features or characteristics in the text lead you to choose each possible classification.' (see Appendix B for the full prompt and output for this task).
******** We then rate the model's output on a scale from 0 to 1 on how closely it matches the classification rule we specified at the beginning of step 1.
******** The subjectivity of this rating step is an important limitation of the current study; more rigorous procedures for future work will be described later. We believe our results still hold value, in part because these classification tasks are sufficiently simple and concrete that the ambiguity is limited.
***** Results
****** We find a range of classification-from-example capability in the model tested, varying substantially by task. TODO rephrase
****** Classification success is as low as 65% and frequently as high as 100%. Note that all these classification tasks are ones that the same model was able to successfully generate cases for (with < 5 errors), so very high failure rates were unlikely.
****** The model's ability to articulate the rule also varied widely, from complete failure to complete success. Success in articulation varied with classification accuracy, with a correlation coefficient of 0.48.
****** The level of false negatives also varied with classification accuracy, with a correlation coefficient of -0.31.
****** Although time (and limited cases of false negative production) do not permit more sophisticated statistical analysis, it seems likely that all three variables are related; both better classification and decreased false negatives during generation indicate cases where the model will be more likely to correctly articulate the underlying rule.

TODO table
***** Discussion
****** Although the data obtained are somewhat limited, it is clear that there are cases where a language model can, with high accuracy, classify test cases using an implicit rule which it is then able to articulate, and also cases in which it classifies successfully but fails to correctly articulate. There are two key questions to consider. First, how faithful are the articulated rules to the ground truth? Second, what are the most accurate and useful framings for the cases where the two do not match?
******* Faithfulness
******** 'Let’s say your LLM successfully articulates a set of rules that it learns in context. (In other words, it succeeds at Step 1 and Step 2). Does that mean the Step 2 faithfully explains the LLM’s behavior in Step 1? What further tests could you do to investigate that?'
********* Philosophical thoughts
********** In a strict sense, we know that the explanation is not necessarily faithful, because the model has no access to its internal state at the actual time of classification. We could change what's in the context, showing that the model made different choices than it did, and it would presumably come up with a plausible explanation to cover *those* choices. Of course, at explanation time (assuming we haven't edited the context), the model is *recreating* its internal state at classification time while processing those tokens, and *that* internal state can play a causal role in the explanation.
********** One thing that complicates the 'faithfulness' question a bit here is that it's not that we think the articulated rule is primary, and then it's deriving its classification choices from that; rather, on my model, there's some induced internal process that it's using both to classify choices and to articulate a rule.
********** Although of course faithfulness as a term of art is not about whether the explanation *is* the causal source; 'a faithful explanation should accurately reflect the reasoning pro-
cess behind the model’s prediction'. [Towards Faithful Model Explanation in NLP: A Survey, Lyu et al 2024. https://arxiv.org/abs/2209.11326v4]
******** Could we tweak unrelated aspects of the question, see whether that changes the classification behavior, and also see if it changes the explanation?
******** We know in general that large language models' explanations of their behavior can be unfaithful, eg see Turpin et al, [[https://arxiv.org/abs/2305.04388][Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting]], where they bias few-shot prompting so that the answer is always 'a', but the model fails to mention this.
********* Can we do something similar here? Can we have the input meet some general criterion, but *also* always be true (or we could extend to (a))?
********* The problem is that then, to the extent that the model always chooses 'a' over the input that meets the criterion, it's *not* a case where the model succeeds at step 1.
********* But maybe we do it both with and without making them all 'a' and see how behavior differs?
********* Something like:
********** give it a problem where the rule is selecting the topic of the sentence (eg, say, 'animal vegetable or mineral') but ALSO the choice is always 'a'. This would require presenting the problems differently.
********** See which rule it articulates.
********** Then give it problems where only sentiment classification AND ones where only 'a', see if it succeeds with those and whether it changes what rule it articulates.
********* The trouble is that very possibly it'll just get the correct rule on both and articulate it -- at that point they're just two different classification tasks.
******** NOTE here's my overall take. The two parts:
********* Where the LLM does succeed in articulating a set of rules that match the behavior:
********** We should never count on faithfulness
********** But in this situation, we generally expect faithfulness because typically there won't be some *different* rule that would give the same results
*********** Although TODO explicitly test this as discussed above.
********** There have been tools developed for attempting to determine whether an explanation is faithful. Time does not allow a comprehensive exploration of these, but some approaches that might be relevant here include:
*********** Look for cases where the articulated rule might give a different answer from the original rule, and test those cases to see which rule is actually followed.
*********** Generate articulated rules multiple times using slightly different prompts, and check consistency of the articulated rules.
********** Of course a classic problem with explanations is that there is often a tradeoff between faithfulness and interpretability; a fully faithful explanation might be too long or complex to be human-understandable, and hence not a very useful explanation. Intuitively that seems unlikely to come into play here, since the underlying rules are simple.
********** If it were a more complex situation, we could give another instance of the LLM the articulated rule and have it classify based on that (without seeing examples), and seeing if the classification results matched the few-shot-based results. But in this case the rules are simple enough that we can tell more or less at a glance whether the articulated rule matches the rule we decided on a priori. <<classify-from-articulated-rule>>
********* Where the LLM fails in articulating the right rule:
********** We know that it can understand the rule in other contexts, because we used the rule in the first place to get a separate session of the same LLM to generate the test cases and accompanying classifications.
********** But blah blah not dishonestly, failure of introspection.
******* Framing
******** There are (at least) two plausible framings for cases where the model succeeds in classification but fails to correctly articulate the underlying rule: dishonesty and failure of introspection.
********* Dishonesty
********** We know the model *can* understand these rules, because it uses them to generate the tests.
********** 'The most widely accepted definition of lying is the following: “A lie is a statement made by one who does not believe it with the intention that someone else shall be led to believe it”' [7].
*********** We should perhaps be skeptical that the model has incentive to be deceptive about the rule it's following.
********** From what I've found, the ability to perform but not articulate really comes at the borders of the model's capability. For one, despite there being a number of cases where the model is 100% correct at classification, the cases where it can't articulate are ones where it gets less than 100%. Also, these are often cases where there are a couple of false negatives during generation. And finally, even when it fails to find the simple rule, it's often able to articulate something in the right neighborhood.
********** As a result, I'd be hesitant to consider this a form of dishonesty, as opposed to being a limitation of the model's ability to correctly introspect.
********** Of course, since there are an arbitrarily large number of possible rules, it may be that I've just failed to find clearer cases, where the model has no false negatives, classifies 100%, and blatantly fails to articulate the rule. TODO is it still true that I haven't found such cases?
********* Introspection failure
********** Point out that humans also can have classification abilities which they are unable to articulate into a rule (chicken-sexing being one philosophically notorious example[6]), and 'dishonesty' is not typically considered a good framing for this.
******* Additional notes
******** The most common type of articulation failure was that the model consistently reached for *semantic* rules, even when the underlying rule was syntactic (eg the sentence contains a particular word).
******** Interestingly, while generating test case files, both GPT-4o and Claude-3.5-Sonnet were completely unable to generate test cases that properly classified sentences by word length. I had thought that the latest models had largely overcome the problem of counting word length rather than token length (just as they've largely overcome handling capitalization and knowing what letter a word starts with).
******* TODO talk about some specific cases
***** Limitations and future work
****** TODO Important: demonstrate that the articulated goals are unlikely to be the goal that the model actually used during classification, by giving another instance of the LLM the articulated rule and having it classify the same test tasks solely based on that (without seeing examples), and seeing if the classification results matched the few-shot-based results.
****** Lots of possible axes on which these rules can vary; we've only tested a few. Results might be quite different for other types of rules.
****** The successfulness of the rule articulation was judged by the researcher without blinding. If this seemed insufficient, future work could improve on this by giving external reviewers the articulated rule and having them attempt to classify the test cases using only that to see if the articulation is adequate.
****** Arguably it would be better, when asking the model to articulate the classification rule, to show it only its own classification output rather than the few-shot examples that have been provided to it throughout. This was omitted due to time constraints, since we don't expect it to make a difference in practice, but will be changed in future work.
****** Given more time, we would further investigate how many training examples the model needs to see in order to perform classification well (although this likely varies by task complexity) rather than our rather ad hoc choice of (typically) 30 examples. In particular, we're especially interested in cases where the model classifies very well but fails to articulate the underlying rule; a useful experiment design here might be to increase the number of training examples until the model is very close to 100% classification success and only then test its ability to articulate the underlying rule.
****** The model used is not as good at generating test cases as a human would be; for example if the task is "true iff the sentence contains the word 'the'", we see:
******* Excessive positives -- more sentences than we might expect by chance contain the word 'the' multiple times, eg 'The phone range during the meeting.'
******* Artificial-seeming negatives -- negative cases sometimes seem forced, eg 'Weather today is pleasant.'
******* This limitation was considered acceptable because of the advantages of having the same model doing generation and articulation (see TODO ).
******* Methodological note: for simplicity, I'm doing one classification at a time, and then when I ask the model to articulate a rule, it's seeing the test examples and one classification that it itself has made. In principle this could cause trouble if the model then tries to articulate a rule based only on the single example, but in practice it clearly doesn't seem to be doing that.
****** This report was written in some haste; to paraphrase Pascal, I have made the language academic, only because I have not had time to make it plain.
***** References
1. Language Models are Few-Shot Learners, Brown et al 2020. https://arxiv.org/abs/2005.14165
2. Foundational Challenges in Assuring Alignment and Safety of Large Language Models, Anwar et al 2024. http://arxiv.org/abs/2404.09932
3. Looking Inward: Language Models Can Learn About Themselves by Introspection, Binder et al 2024. https://arxiv.org/abs/2410.13787
4. Language Models Can Articulate Their Implicit Goals, Chua et al 2024. Forthcoming.
5. Transformers learn in-context by gradient descent, von Oswald et al 2023. http://arxiv.org/abs/2212.07677
6. Is Introspective Knowledge Incorrigible?, D.M. Armstrong 1963. https://www.jstor.org/stable/2183028
7. The Definition of Lying and Deception, James Edwin Mahon 2016. https://plato.stanford.edu/archives/win2016/entries/lying-definition (Stanford Encyclopedia of Philosophy). Citing 'Deontology and the Ethics of Lying', Arnold Isenberg 1964. https://www.jstor.org/stable/2104756
****** Footnotes
******* * Note: literature search was omitted for this interim report due to time constraints; I'm giving my best guess here.
******* ** Of course, in the typical case of few-shot classification tasks, the task is described before giving examples. Here that would directly describe to the model the rule we want it to independently articulate. Therefore we preliminarily investigate whether current-generation language models are capable of learning a classification task from examples alone, without a description, and find that in fact they are.
**** Appendices
***** Note that full code can be found at https://github.com/eggsyntax/articulating-learned-rules
***** Appendix A: full prompt and output for task generation.
***** Appendix B: full prompt and output for classification.
***** Appendix C: version of figure 1 with notes on each case.
**** TODO
***** TODO Try the experiment of having multiple rules that could fit, see which one it picks.
***** TODO Try the experiment of [[classify-from-articulated-rule]] -- get success figures for those.
***** TODO come up with experiments that would distinguish dishonesty from a failure of introspection.
***** DONE Maybe come up with some different types of rules.
***** TODO Remember to say under methods that I find multiple choice somewhat unconvincing here, because the LLM can succeed at this by evaluating post-hoc which of the answers applies.
***** TODO Remember to include quantitative results as figures or tables
****** 'Think of this as a report you are writing to your research collaborators (not yet for public consumption).'
***** TODO Remember that we can test articulation with multiple choice OR free-form
***** TODO Remember to create requirements.txt
****** conda env export > environment.yml --no-builds
****** conda list -e > requirements.txt
****** pip freeze > requirements.txt
******* Or if strange paths, do pip list --format=freeze > requirements.txt
***** DONE Remember to create github repo
***** TODO mention any confusions
**** Thoughts
***** DONE Are there *any* cases where a model can perform but not articulate?
***** DONE Better to come from the easy side or the hard side or the middle?
****** On the easy side we start with cases that the model can do and articulate
****** On the hard side we start with cases that the model can neither do nor articulate
****** Probably best to start in the middle and do a rough binary search
****** Maybe checking both ends first?
****** 'Output is a length' is the easiest one to modulate difficulty
***** DONE How much automation?
****** Two days is not a lot of time
****** Maybe start by looking for signs of life manually?
**** Some ideas for goals
***** Input is of a length (NB: tokens or words?)
****** Input is of a length which is a Fibonacci number
****** Input is of a length which is some less-well-known sequence
****** Input is of a length which is a member of an arbitrary invented sequence
****** Input is of a length which is a function f(input_length), with varying levels of complexity for f
***** Input contains a word
****** Input contains a particular word, eg 'diaphanous'
****** Input contains a word from a set of words of length n << num_finetuning_examples
****** Input contains a word
***** Input letters (interesting one because no overt access to spelling -- except there is the spelling miracle)
****** All input words start with the letter L
****** Some input word starts with the letter L
****** All input words start with a vowel
****** All input words start with a letter which is part of an arbitrary set
****** Input contains a prime number of vowels (h/t EY)
***** Semantic
****** The input mentions animals
***** Affective
****** The input conveys grumpiness
****** The input conveys happiness
**** Hours spent
***** Tuesday: 7.25
***** Wednesday: 2.5
***** Thursday: 3.0
***** Note that I did jot down a few notes at times when I wasn't officially working on this; they just occurred to me in between work sessions and I didn't want to lose track of them.
